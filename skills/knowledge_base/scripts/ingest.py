import os
import sys
import glob
import shutil
import hashlib

# [å…³é”®ä¿®å¤] å…ˆè®¡ç®—å¹¶æ·»åŠ é¡¹ç›®æ ¹ç›®å½•ï¼Œå†è¿›è¡Œåç»­ import
CURRENT_DIR = os.path.dirname(os.path.abspath(__file__))
PROJECT_ROOT = os.path.dirname(os.path.dirname(os.path.dirname(CURRENT_DIR)))
if PROJECT_ROOT not in sys.path:
    sys.path.append(PROJECT_ROOT)

# ç°åœ¨å¯ä»¥å®‰å…¨åœ°å¯¼å…¥äº†
from skills.knowledge_base.scripts.db_manager import DBManager, DOCS_ARCHIVE_PATH
from agent_core.tools import read_file

def chunk_text_by_lines(text, chunk_size=20, overlap=5):
    """
    æŒ‰è¡Œåˆ‡åˆ†æ–‡æœ¬ï¼Œå¹¶å°è¯•æå–è¯­ä¹‰åŒ–çš„ä½ç½®ä¿¡æ¯ï¼ˆå¦‚ Slide 1, Page 2, Sheet Nameï¼‰ã€‚
    è¿”å›: List[dict] -> [{'text': '...', 'lines': '10-30', 'location': 'Slide 5'}]
    """
    lines = text.splitlines()
    chunks = []
    total_lines = len(lines)
    
    # é¢„æ‰«æï¼šå»ºç«‹è¡Œå·åˆ°ä½ç½®çš„æ˜ å°„
    line_location_map = {}
    current_location = "Unknown Location"
    
    import re
    # åŒ¹é…æ¨¡å¼: --- Slide 1 ---, --- Page 1 ---, --- Sheet: Sheet1 ---
    loc_pattern = re.compile(r'^--- (Slide \d+|Page \d+|Sheet: .+) ---$')
    
    for i, line in enumerate(lines):
        match = loc_pattern.match(line.strip())
        if match:
            current_location = match.group(1)
        line_location_map[i] = current_location
    
    for i in range(0, total_lines, chunk_size - overlap):
        end = min(i + chunk_size, total_lines)
        chunk_lines = lines[i:end]
        chunk_content = "\n".join(chunk_lines).strip()
        
        if not chunk_content: continue
        
        start_loc = line_location_map.get(i, "Unknown")
        end_loc = line_location_map.get(end-1, "Unknown")
        
        if start_loc == end_loc:
            location = start_loc
        else:
            location = f"{start_loc} -> {end_loc}"
            
        chunks.append({
            "text": chunk_content,
            "line_start": i + 1,
            "line_end": end,
            "location": location
        })
        
        if end == total_lines: break
        
    return chunks

def archive_file(file_path):
    """å°†æ–‡ä»¶å½’æ¡£åˆ°å½±å­ç›®å½•ï¼Œè¿”å›å½’æ¡£åçš„ç»å¯¹è·¯å¾„"""
    try:
        with open(file_path, "rb") as f:
            file_hash = hashlib.md5(f.read()).hexdigest()[:8]
        
        filename = os.path.basename(file_path)
        new_filename = f"{file_hash}_{filename}"
        new_path = os.path.join(DOCS_ARCHIVE_PATH, new_filename)
        
        if not os.path.exists(new_path):
            shutil.copy2(file_path, new_path)
            print(f"ğŸ“¦ Archived to: {new_path}")
        else:
            print(f"ğŸ“¦ Used existing archive: {new_path}")
            
        return new_path
    except Exception as e:
        print(f"âš ï¸ Archive failed: {e}. Using original path.")
        return file_path

def ingest_file(file_path, collection_name="documents"):
    # 1. å½’æ¡£ (Copy-on-Ingest)
    target_path = archive_file(file_path)
    print(f"ğŸ“„ Processing: {target_path}")
    
    # 2. è°ƒç”¨ Core Tool è¯»å–æ–‡ä»¶ (ä½¿ç”¨å½’æ¡£åçš„è·¯å¾„)
    full_content = ""
    start_line = 1
    page_size = 1000 
    
    while True:
        # ä½¿ç”¨ read_file.func ç›´æ¥è°ƒç”¨
        part = read_file.func(target_path, start_line=start_line, end_line=start_line + page_size)
        
        body = part
        if "--- æ–‡ä»¶å…ƒæ•°æ® ---" in part:
            body = part.split("--- æ–‡ä»¶å…ƒæ•°æ® ---")[1].split("\n", 4)[-1]
        if "[SYSTEM WARNING]" in body:
            body = body.split("[SYSTEM WARNING]")[0]
            
        full_content += body
        
        if "[SYSTEM WARNING]" not in part: 
            break
        start_line += page_size
        if start_line > 10000:
            print("âš ï¸ File too large (>10k lines), stopping.")
            break

    # 3. åˆ‡ç‰‡
    chunks = chunk_text_by_lines(full_content)
    print(f"   -> Split into {len(chunks)} chunks.")
    
    if not chunks: return

    # 4. å‘é‡åŒ– & å­˜å‚¨
    db = DBManager.get_instance()
    vectors = db.embed_documents([c['text'] for c in chunks])
    
    # ä½¿ç”¨å½’æ¡£è·¯å¾„ä½œä¸º Source
    final_source = target_path

    data = []
    for i, chunk in enumerate(chunks):
        data.append({
            "vector": vectors[i],
            "text": chunk['text'],
            "source": final_source, 
            "line_range": f"{chunk['line_start']}-{chunk['line_end']}",
            "location": chunk['location'], 
            "type": "document"
        })
        
    # 5. å†™å…¥ DB
    is_compatible = db.check_schema_compatibility(collection_name, data[0])
    
    tbl = db.get_table(collection_name)
    if tbl and is_compatible:
        tbl.add(data)
    else:
        db.create_table(collection_name, data)
        
    print(f"âœ… Ingested {len(data)} vectors to '{collection_name}'.")

def main(input_path, collection="documents"):
    if os.path.isfile(input_path):
        ingest_file(input_path, collection)
    elif os.path.isdir(input_path):
        # é€’å½’æŸ¥æ‰¾æ”¯æŒçš„æ ¼å¼
        exts = ['*.docx', '*.pdf', '*.xlsx', '*.pptx', '*.md', '*.txt']
        files = []
        for ext in exts:
            files.extend(glob.glob(os.path.join(input_path, '**', ext), recursive=True))
            
        print(f"ğŸ” Found {len(files)} files in {input_path}")
        for f in files:
            try:
                ingest_file(f, collection)
            except Exception as e:
                print(f"âŒ Error processing {f}: {e}")

if __name__ == "__main__":
    if len(sys.argv) < 2:
        print("Usage: python ingest.py <file_or_dir> [collection_name]")
        sys.exit(1)
    
    target = sys.argv[1]
    coll = sys.argv[2] if len(sys.argv) > 2 else "documents"
    main(target, coll)